{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94910989-2db2-4cd6-baa6-7ca1a3567da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Databricks Getting Started - Essential Demos Installation\n",
    "\n",
    "Welcome to Databricks! This notebook will help you get started by installing and exploring the most important demos that showcase key platform capabilities.\n",
    "\n",
    "## What You'll Install\n",
    "\n",
    "This notebook installs 5 essential **dbdemos** that every new Databricks user should explore:\n",
    "\n",
    "* **Delta Lake** - Learn about the foundation of the Databricks Lakehouse with ACID transactions, time travel, and data versioning\n",
    "* **dbt on Databricks** - Discover how to orchestrate and run your dbt jobs seamlessly on Databricks\n",
    "* **Auto Loader** - Master incremental data ingestion from cloud storage with automatic schema evolution\n",
    "* **AI/BI Portfolio Assistant** - Explore advanced analytics and AI capabilities with dashboards and Genie\n",
    "* **SQL Warehouse** - Understand data warehousing features including identity columns, primary/foreign keys, and stored procedures\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Databricks workspace access\n",
    "* Cluster with appropriate permissions\n",
    "* Internet connectivity for package installation\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. Run each cell sequentially\n",
    "2. Each demo installation may take 2-5 minutes\n",
    "3. After installation, explore the generated folders in your workspace\n",
    "4. Follow the README files in each demo folder for detailed walkthroughs\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Note**: Demo installations will create new folders, tables, and resources in your workspace. Make sure you have sufficient permissions and storage quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b6c579-60c9-42c9-a2a6-243d2816d953",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dbdemos Package"
    }
   },
   "outputs": [],
   "source": [
    "# Install the dbdemos package\n",
    "# This package provides pre-built demos showcasing Databricks capabilities\n",
    "%pip install dbdemos --quiet\n",
    "\n",
    "# Restart Python to ensure the package is properly loaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40994e07-ddf0-4f0f-ab30-89729db0fc5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import dbdemos and Check Version"
    }
   },
   "outputs": [],
   "source": [
    "import dbdemos\n",
    "\n",
    "# Display dbdemos version and available demos\n",
    "print(f\"dbdemos version: {dbdemos.__version__}\")\n",
    "print(\"\\nüìã Installing 5 essential demos for new Databricks users...\")\n",
    "print(\"Each installation may take 2-5 minutes depending on demo complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd4e465-285f-4374-9345-55257ad50ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üèóÔ∏è Demo 1: Delta Lake - The Foundation of Databricks Lakehouse\n",
    "\n",
    "**Delta Lake** is the storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
    "\n",
    "### What you'll learn:\n",
    "* ACID transactions for data reliability\n",
    "* Time travel and data versioning\n",
    "* Schema enforcement and evolution\n",
    "* Optimizations like Z-ordering and auto-compaction\n",
    "* Streaming and batch data processing\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "* Creating Delta tables\n",
    "* Handling schema changes\n",
    "* Time travel queries\n",
    "* Merge operations (UPSERT)\n",
    "* Performance optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817c085a-00f9-4816-904e-b1cf7c3da827",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Delta Lake Demo"
    }
   },
   "outputs": [],
   "source": [
    "# Install Delta Lake demo\n",
    "print(\"üîÑ Installing Delta Lake demo...\")\n",
    "dbdemos.install('delta-lake')\n",
    "print(\"‚úÖ Delta Lake demo installed successfully!\")\n",
    "print(\"üìÅ Check the 'delta-lake' folder in your workspace for notebooks and datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "031d666a-17dd-4c79-ab29-2439ec02cea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîß Demo 2: dbt on Databricks - Modern Data Transformation\n",
    "\n",
    "**dbt (data build tool)** enables analytics engineers to transform data using SQL and software engineering best practices.\n",
    "\n",
    "### What you'll learn:\n",
    "* Setting up dbt projects on Databricks\n",
    "* Creating dbt models and transformations\n",
    "* Testing and documentation\n",
    "* Orchestrating dbt jobs\n",
    "* Integration with Databricks workflows\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "* dbt model development\n",
    "* Data quality testing\n",
    "* Incremental models\n",
    "* Macros and packages\n",
    "* Job orchestration and scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95132567-51b8-4327-9192-783734fb4ba2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install dbt Demo"
    }
   },
   "outputs": [],
   "source": [
    "# Install dbt on Databricks demo\n",
    "print(\"üîÑ Installing dbt on Databricks demo...\")\n",
    "dbdemos.install('dbt-on-databricks')\n",
    "print(\"‚úÖ dbt on Databricks demo installed successfully!\")\n",
    "print(\"üìÅ Check the 'dbt-on-databricks' folder in your workspace for dbt projects and workflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc09a88c-00b4-4e73-b944-f39ace689b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì• Demo 3: Auto Loader - Incremental Data Ingestion\n",
    "\n",
    "**Auto Loader** incrementally and efficiently processes new data files as they arrive in cloud storage.\n",
    "\n",
    "### What you'll learn:\n",
    "* Setting up Auto Loader for various file formats\n",
    "* Automatic schema inference and evolution\n",
    "* Handling bad records and data quality\n",
    "* Monitoring and alerting\n",
    "* Integration with Delta Live Tables\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "* Cloud file ingestion (S3, ADLS, GCS)\n",
    "* Schema evolution handling\n",
    "* Checkpointing and exactly-once processing\n",
    "* Error handling and dead letter queues\n",
    "* Performance optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdeb2b1-eb89-4f63-b52c-f0d8652f7123",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Auto Loader Demo"
    }
   },
   "outputs": [],
   "source": [
    "# Install Auto Loader demo\n",
    "print(\"üîÑ Installing Auto Loader demo...\")\n",
    "dbdemos.install('auto-loader')\n",
    "print(\"‚úÖ Auto Loader demo installed successfully!\")\n",
    "print(\"üìÅ Check the 'auto-loader' folder in your workspace for ingestion patterns and examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c78e5830-7d9c-46e4-98bf-8fd54b2735c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü§ñ Demo 4: AI/BI Portfolio Assistant - Advanced Analytics & AI\n",
    "\n",
    "**Databricks AI/BI** combines the power of AI with business intelligence for advanced analytics in capital markets.\n",
    "\n",
    "### What you'll learn:\n",
    "* Building AI-powered dashboards\n",
    "* Using Genie for natural language queries\n",
    "* Financial data analysis and modeling\n",
    "* Real-time portfolio monitoring\n",
    "* Advanced visualization techniques\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "* AI-assisted data exploration\n",
    "* Natural language to SQL with Genie\n",
    "* Interactive dashboards\n",
    "* Financial risk modeling\n",
    "* Automated insights and alerts\n",
    "\n",
    "**Note**: This demo uses a custom catalog and schema for financial services data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da66102-53c6-413f-a3f7-bf8d147ee8e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install AI/BI Demo"
    }
   },
   "outputs": [],
   "source": [
    "# Install AI/BI Portfolio Assistant demo with custom catalog and schema\n",
    "print(\"üîÑ Installing AI/BI Portfolio Assistant demo...\")\n",
    "dbdemos.install('aibi-portfolio-assistant', catalog='main', schema='dbdemos_aibi_fsi_portfolio_assistant')\n",
    "print(\"‚úÖ AI/BI Portfolio Assistant demo installed successfully!\")\n",
    "print(\"üìÅ Check the 'aibi-portfolio-assistant' folder for dashboards and AI-powered analytics.\")\n",
    "print(\"üóÑÔ∏è Data stored in: main.dbdemos_aibi_fsi_portfolio_assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5663b7f-b8a6-4e17-a533-bcac09ec21d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üè¢ Demo 5: SQL Warehouse - Enterprise Data Warehousing\n",
    "\n",
    "**SQL Warehouse** demonstrates advanced data warehousing capabilities including modern SQL features and enterprise-grade functionality.\n",
    "\n",
    "### What you'll learn:\n",
    "* Identity columns and auto-incrementing keys\n",
    "* Primary and foreign key constraints\n",
    "* Stored procedures and functions\n",
    "* Control flow with loops and conditionals\n",
    "* Advanced SQL patterns and optimizations\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "* Table constraints and relationships\n",
    "* Stored procedure development\n",
    "* Transaction management\n",
    "* Performance tuning\n",
    "* Data governance and security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0799ee4-1048-4c72-8018-26819eefc215",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install SQL Warehouse Demo"
    }
   },
   "outputs": [],
   "source": [
    "# Install SQL Warehouse demo\n",
    "print(\"üîÑ Installing SQL Warehouse demo...\")\n",
    "dbdemos.install('sql-warehouse')\n",
    "print(\"‚úÖ SQL Warehouse demo installed successfully!\")\n",
    "print(\"üìÅ Check the 'sql-warehouse' folder for advanced SQL examples and stored procedures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ebb28a-801e-4d5a-b1d4-b87f7bfb5eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéâ Installation Complete!\n",
    "\n",
    "Congratulations! You've successfully installed 5 essential Databricks demos. Here's what to do next:\n",
    "\n",
    "## üìÇ Explore Your New Demo Folders\n",
    "\n",
    "Check your workspace for these new folders:\n",
    "* `delta-lake/` - Delta Lake fundamentals and advanced features\n",
    "* `dbt-on-databricks/` - dbt transformation workflows\n",
    "* `auto-loader/` - Data ingestion patterns\n",
    "* `aibi-portfolio-assistant/` - AI/BI analytics and dashboards\n",
    "* `sql-warehouse/` - Advanced SQL warehousing features\n",
    "\n",
    "## üöÄ Recommended Learning Path\n",
    "\n",
    "1. **Start with Delta Lake** - Understanding the storage foundation\n",
    "2. **Explore Auto Loader** - Learn data ingestion patterns\n",
    "3. **Try dbt on Databricks** - Modern data transformation\n",
    "4. **Experiment with SQL Warehouse** - Advanced SQL features\n",
    "5. **Dive into AI/BI** - Advanced analytics and AI capabilities\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "* [Databricks Documentation](https://docs.databricks.com/)\n",
    "* [Databricks Academy](https://academy.databricks.com/)\n",
    "* [Community Forums](https://community.databricks.com/)\n",
    "* [GitHub Examples](https://github.com/databricks)\n",
    "\n",
    "## üí° Tips for Success\n",
    "\n",
    "* Each demo folder contains a README with detailed instructions\n",
    "* Start with the `00-` numbered notebooks in each folder\n",
    "* Don't hesitate to modify and experiment with the code\n",
    "* Join the Databricks community for support and best practices\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üéì**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08251150-2dda-40b4-95c2-9aad24c4e4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Databricks Getting Started - Essential Demos Installation\n",
    "\n",
    "Welcome to Databricks! This notebook will help you get started by installing and exploring the most important demos that showcase key platform capabilities.\n",
    "\n",
    "## What You'll Install\n",
    "\n",
    "This notebook installs 5 essential **dbdemos** that every new Databricks user should explore:\n",
    "\n",
    "* **Delta Lake** - Learn about the foundation of the Databricks Lakehouse with ACID transactions, time travel, and data versioning\n",
    "* **dbt on Databricks** - Discover how to orchestrate and run your dbt jobs seamlessly on Databricks\n",
    "* **Auto Loader** - Master incremental data ingestion from cloud storage with automatic schema evolution\n",
    "* **AI/BI Portfolio Assistant** - Explore advanced analytics and AI capabilities with dashboards and Genie\n",
    "* **SQL Warehouse** - Understand data warehousing features including identity columns, primary/foreign keys, and stored procedures\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Databricks workspace access\n",
    "* Cluster with appropriate permissions\n",
    "* Internet connectivity for package installation\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. Run each cell sequentially\n",
    "2. Each demo installation may take 2-5 minutes\n",
    "3. After installation, explore the generated folders in your workspace\n",
    "4. Follow the README files in each demo folder for detailed walkthroughs\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Note**: Demo installations will create new folders, tables, and resources in your workspace. Make sure you have sufficient permissions and storage quota."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Getting Started",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
