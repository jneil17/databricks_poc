{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698b6c64-c1f2-483f-96e9-79f6773edc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Getting Data Into Databricks\n",
    "\n",
    "Every data journey starts with ingestion. Databricks is an open platform that provides multiple ways to load your data, from simple point-and-click interfaces to automated, enterprise-grade tools. Let's explore three common methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f64de1e-ddd6-4205-9867-8be4a8419da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 1: Point-and-Click with Lakeflow Connect\n",
    "\n",
    "The easiest way to connect to hundreds of data sources is using the built-in connectors. This UI-driven approach is perfect for quickly loading data from external databases like MySQL, Postgres, Salesforce, and many more without writing code.\n",
    "\n",
    "The video below provides a fantastic overview of how to connect to sources using the UI.\n",
    "\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/7uHLVQSHVAw/0.jpg)](https://www.youtube.com/watch?v=7uHLVQSHVAw&t=1s)\n",
    "\n",
    "### To Try It Yourself:\n",
    "1. In the left navigation bar, click **+ New** > **Add data**.\n",
    "2. Click **Lakeflow Connect** to browse available sources.\n",
    "\n",
    "üìñ **Resource:** [Lakeflow Connect Documentation](https://docs.databricks.com/en/connect/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c0681e-64b9-458e-a2f1-c270483e0a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 2: Partner Connect with Fivetran & Others\n",
    "\n",
    "Databricks partners with leading data integration companies like Fivetran, Airbyte, and Informatica. These partners provide pre-built, managed connectors for hundreds of SaaS applications (e.g., Google Analytics, Stripe, HubSpot).\n",
    "\n",
    "**Partner Connect** is a feature in the Databricks UI that simplifies the process of connecting these tools to your workspace.\n",
    "\n",
    "### To Explore:\n",
    "1. In the left navigation bar, go to **Partner Connect**.\n",
    "2. Find a data integration partner like **Fivetran** and follow the on-screen steps to connect.\n",
    "\n",
    "üìñ **Resource:** [Partner Connect Documentation](https://docs.databricks.com/en/partner-connect/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c91691-b7a5-408c-8645-ec82dae75e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 3: Automated Ingestion with Auto Loader\n",
    "\n",
    "For data landing in cloud storage (S3, ADLS, GCS), **Auto Loader** is the most powerful and efficient tool. It automatically and incrementally processes new files as they arrive, handling schema changes and scalability for you. This is the recommended best practice for file-based ingestion.\n",
    "\n",
    "Your setup script installed the `auto-loader` demo, which provides a hands-on example.\n",
    "\n",
    "### To See it in Action:\n",
    "\n",
    "Navigate to the `auto-loader` demo folder in your workspace and open the **`01-Auto-Loader-and-Schema-Evolution`** notebook. This notebook shows how you can easily ingest files with just a few lines of SQL or Python.\n",
    "\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/2F6mBvLoavs/0.jpg)](https://www.youtube.com/watch?v=2F6mBvLoavs&t=1s)\n",
    "\n",
    "### üìñ Additional Resources:\n",
    "* [Auto Loader Documentation](https://docs.databricks.com/en/ingestion/auto-loader/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ba1482-1435-4ddb-802a-9aec89146830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Method 4: Code-Based Ingestion Examples\n",
    "\n",
    "For developers who prefer programmatic approaches, here are code examples for common ingestion patterns:\n",
    "\n",
    "### Reading from Cloud Storage (S3, ADLS, GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80def5d6-76de-4d2f-a371-af517ddf1aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Reading CSV files from cloud storage\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://your-bucket/path/to/files/*.csv\")\n",
    "\n",
    "# Save to Delta table\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.default.your_table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74bae5f5-cd35-4421-8ce7-2ed4f309a068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Auto Loader with SQL (Recommended for Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769c886c-5844-42e9-a0c3-e744f64ebf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Auto Loader example: Automatically ingest new files as they arrive\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE raw_data\n",
    "AS SELECT * FROM cloud_files(\n",
    "  \"s3://your-bucket/incoming-data/\", \n",
    "  \"json\",\n",
    "  map(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f05f461-60b2-49a0-9f0a-518c168a3f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comprehensive Resource Library\n",
    "\n",
    "### üìö **Official Documentation**\n",
    "* [Data Ingestion on Databricks - Complete Guide](https://docs.databricks.com/en/ingestion/index.html)\n",
    "* [Auto Loader Deep Dive](https://docs.databricks.com/en/ingestion/auto-loader/index.html)\n",
    "* [Partner Connect Documentation](https://docs.databricks.com/en/partner-connect/index.html)\n",
    "* [Lakehouse Connect (Data Connectors)](https://docs.databricks.com/en/connect/index.html)\n",
    "* [File Upload and Data Import](https://docs.databricks.com/en/ingestion/add-data/index.html)\n",
    "\n",
    "### üõ†Ô∏è **Hands-On Tutorials**\n",
    "* [Data Engineering with Databricks Course](https://www.databricks.com/learn/training/data-engineering-courses)\n",
    "\n",
    "### üìñ **Advanced Reading**\n",
    "* [Schema Evolution in Auto Loader](https://docs.databricks.com/en/ingestion/auto-loader/schema.html)\n",
    "* [Real-time Streaming Ingestion Patterns](https://docs.databricks.com/en/structured-streaming/index.html)\n",
    "\n",
    "### üèóÔ∏è **Architecture Patterns**\n",
    "* [Medallion Architecture for Data Ingestion](https://www.databricks.com/glossary/medallion-architecture)\n",
    "\n",
    "### üí° **Community Resources**\n",
    "* [Reddit - r/databricks](https://www.reddit.com/r/databricks/)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8684803394761513,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
