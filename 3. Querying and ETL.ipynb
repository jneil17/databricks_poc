{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "137a6896-f531-4ef9-b519-b12c28b0c912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Querying and Transforming Your Data (ETL)\n",
    "\n",
    "Once your data is in the Lakehouse, you need to query it and build transformation pipelines. Databricks offers two primary methods for orchestrating this work:\n",
    "\n",
    "1. **Databricks Jobs:** An imperative approach, ideal for simple, scheduled tasks.\n",
    "2. **Delta Live Tables:** A declarative approach, ideal for building robust, observable data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Queries in the Databricks SQL Editor\n",
    "\n",
    "The **SQL Editor** is your home for data exploration. You can write standard SQL to query any table, and the underlying Serverless SQL Warehouse provides best-in-class performance. You can also review the query profile to troubleshoot performance bottlenecks and see query history.\n",
    "\n",
    "### To Try It Yourself:\n",
    "1. In the left navigation bar, select **SQL Editor**.\n",
    "2. Ensure a SQL Warehouse is selected in the top-right.\n",
    "3. Run the query below to explore the clean, aggregated data produced by one of the demo pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- This is the final 'gold' table from the bike pipeline demo, ready for BI.\n",
    "SELECT * FROM main.dbdemos_pipeline_bike.bike_trips_gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration Method 1: Databricks Jobs (The Imperative Approach)\n",
    "\n",
    "A **Job** is the simplest way to run a notebook, script, or SQL query on a schedule. This is an *imperative* approach, where you define the steps to be executed in order. Think of it as a powerful \"cron job\" for Databricks, perfect for straightforward, routine tasks.\n",
    "\n",
    "### To See How it Works:\n",
    "\n",
    "You can schedule any notebook to run as a Job by clicking the **Schedule** button in the top-right corner of the notebook UI. This will take you to the Jobs UI where you can define the schedule and compute.\n",
    "\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/gHye-4w_d7A/0.jpg)](https://www.youtube.com/watch?v=gHye-4w_d7A \"Databricks Workflows: Build, Run, and Manage ETL, ML, and Analytics\")\n",
    "\n",
    "ðŸ“– **Resource:** [Databricks Workflows Quickstart](https://docs.databricks.com/en/workflows/jobs/jobs-quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration Method 2: Delta Live Tables (The Declarative Approach)\n",
    "\n",
    "For building robust data pipelines, **Delta Live Tables (DLT)** is the modern, recommended approach. It's a *declarative* framework: instead of defining the *steps* of your pipeline, you simply define the *end state* of your tables using standard SQL or Python.\n",
    "\n",
    "DLT automatically manages the underlying infrastructure, orchestration, data quality monitoring, and error handling.\n",
    "\n",
    "Your setup script already deployed a DLT pipeline from the `pipeline-bike` demo!\n",
    "\n",
    "### To Explore the DLT Pipeline:\n",
    "\n",
    "1. **See the Definition:** In your workspace, navigate to the `pipeline-bike` demo folder and open the **`01-DLT-Pipeline-SQL`** notebook. This is the simple SQL code that defines the entire pipeline.\n",
    "2. **See it Running:** Use the link generated by the setup script (or go to `Workflows > Delta Live Tables`) to see the live pipeline graph. You can monitor data flowing through the bronze, silver, and gold tables and see data quality scores.\n",
    "\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/1LAL-8y_q9Y/0.jpg)](https://www.youtube.com/watch?v=1LAL-8y_q9Y \"What is Delta Live Tables?\")\n",
    "\n",
    "### ðŸ“– Additional Resources:\n",
    "* [Delta Live Tables Quickstart](https://docs.databricks.com/en/delta-live-tables/quickstart.html)\n",
    "* [Tutorial: Run your first ETL pipeline with DLT](https://docs.databricks.com/en/delta-live-tables/tutorial-run-pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced SQL Techniques and Performance Optimization\n",
    "\n",
    "### Query Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Example: Using Z-ordering for optimal query performance\n",
    "OPTIMIZE main.default.your_table_name\n",
    "ZORDER BY (frequently_filtered_column);\n",
    "\n",
    "-- Example: Using partitioning for large tables\n",
    "CREATE TABLE main.default.sales_data (\n",
    "    transaction_id STRING,\n",
    "    amount DECIMAL(10,2),\n",
    "    customer_id STRING,\n",
    "    sale_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (sale_date);\n",
    "\n",
    "-- Example: Analyzing query performance\n",
    "EXPLAIN EXTENDED \n",
    "SELECT customer_id, SUM(amount) as total_sales\n",
    "FROM main.default.sales_data \n",
    "WHERE sale_date >= '2024-01-01'\n",
    "GROUP BY customer_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality and Testing with Delta Live Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Example: Delta Live Tables with data quality constraints\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE clean_customer_data (\n",
    "  CONSTRAINT valid_email EXPECT (email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$') ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_age EXPECT (age > 0 AND age < 150) ON VIOLATION QUARANTINE,\n",
    "  CONSTRAINT not_null_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS SELECT \n",
    "  customer_id,\n",
    "  email,\n",
    "  age,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM(LIVE.raw_customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Resource Library\n",
    "\n",
    "### ðŸ“š **Official Documentation**\n",
    "* [SQL Analytics and Warehousing Complete Guide](https://docs.databricks.com/en/sql/index.html)\n",
    "* [Delta Live Tables Documentation](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "* [Databricks Workflows (Jobs) Guide](https://docs.databricks.com/en/workflows/index.html)\n",
    "* [SQL Reference Guide](https://docs.databricks.com/en/sql/language-manual/index.html)\n",
    "* [Query Optimization Best Practices](https://docs.databricks.com/en/optimizations/index.html)\n",
    "* [Data Quality Monitoring](https://docs.databricks.com/en/delta-live-tables/expectations.html)\n",
    "\n",
    "### ðŸŽ¥ **Video Learning Resources**\n",
    "* [SQL Analytics Fundamentals Playlist](https://www.youtube.com/playlist?list=PLTPXxbhUt-YVstcW1CG5F0S3LXvvfRT1u)\n",
    "* [Delta Live Tables Deep Dive](https://www.youtube.com/watch?v=1LAL-8y_q9Y)\n",
    "* [Advanced SQL Techniques on Databricks](https://www.youtube.com/watch?v=LWtj-84Hi8E)\n",
    "* [Performance Tuning for Analytics](https://www.youtube.com/watch?v=FfVuCpMhV6Q)\n",
    "* [Data Quality with DLT](https://www.youtube.com/watch?v=vv8OPIhCGE8)\n",
    "\n",
    "### ðŸ› ï¸ **Hands-On Tutorials and Labs**\n",
    "* [Delta Live Tables Quickstart](https://docs.databricks.com/en/delta-live-tables/quickstart.html)\n",
    "* [SQL Analytics Workshop](https://github.com/databricks-academy/sql-analytics-with-databricks)\n",
    "* [Advanced Analytics with Databricks Course](https://academy.databricks.com/path/data-analyst)\n",
    "* [ETL with Delta Live Tables Tutorial](https://docs.databricks.com/en/delta-live-tables/tutorial-run-pipeline.html)\n",
    "* [Building Production Pipelines](https://github.com/databricks-academy/advanced-data-engineering-with-databricks)\n",
    "\n",
    "### ðŸ“– **Advanced Reading and Best Practices**\n",
    "* [Medallion Architecture Implementation](https://www.databricks.com/glossary/medallion-architecture)\n",
    "* [SQL Performance Optimization Guide](https://www.databricks.com/blog/2021/08/11/high-performance-sql-analytics-with-databricks-sql.html)\n",
    "* [Data Pipeline Testing Strategies](https://www.databricks.com/blog/2021/12/09/testing-data-pipelines-with-databricks.html)\n",
    "* [Change Data Capture (CDC) Patterns](https://docs.databricks.com/en/delta-live-tables/cdc.html)\n",
    "* [Streaming Analytics Best Practices](https://www.databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html)\n",
    "\n",
    "### ðŸ—ï¸ **Architecture and Design Patterns**\n",
    "* [Modern Data Stack with Databricks](https://www.databricks.com/blog/2021/08/30/frequently-asked-questions-about-the-databricks-lakehouse-platform.html)\n",
    "* [Real-time Analytics Architecture](https://www.databricks.com/solutions/accelerators/real-time-analytics)\n",
    "* [Multi-layer Data Architecture](https://www.databricks.com/blog/2020/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html)\n",
    "* [Scaling ETL Workloads](https://www.databricks.com/blog/2021/05/24/how-to-scale-machine-learning-pipelines.html)\n",
    "\n",
    "### ðŸ”§ **Tools and Integrations**\n",
    "* [dbt Integration with Databricks](https://docs.databricks.com/en/partners/prep/dbt.html)\n",
    "* [Apache Airflow Integration](https://docs.databricks.com/en/dev-tools/external-tools.html#apache-airflow)\n",
    "* [GitHub Actions for CI/CD](https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-github.html)\n",
    "* [Terraform for Infrastructure](https://registry.terraform.io/providers/databricks/databricks/latest/docs)\n",
    "\n",
    "### ðŸ’¡ **Community and Support**\n",
    "* [Databricks Community Forums - SQL & Analytics](https://community.databricks.com/s/topic/0TO0w000000MqzEGAS/sql-analytics)\n",
    "* [Delta Live Tables Community Discussions](https://community.databricks.com/s/topic/0TO5w00000096DHGAY/delta-live-tables)\n",
    "* [Stack Overflow - Databricks SQL](https://stackoverflow.com/questions/tagged/databricks+sql)\n",
    "* [LinkedIn Learning Databricks Courses](https://www.linkedin.com/learning/search?keywords=databricks)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Querying and ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
