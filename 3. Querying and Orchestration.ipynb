{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137a6896-f531-4ef9-b519-b12c28b0c912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Querying and Transforming Your Data (ETL)\n",
    "\n",
    "Once your data is in the Lakehouse, you need to query it and build transformation pipelines. Databricks offers two primary methods for orchestrating this work:\n",
    "\n",
    "1. **Databricks Jobs:** An imperative approach, ideal for simple, scheduled tasks.\n",
    "2. **Delta Live Tables:** A declarative approach, ideal for building robust, observable data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4a3255-b432-4539-9850-181a643b8358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing Queries in the Databricks SQL Editor\n",
    "\n",
    "The **SQL Editor** is your home for data exploration. You can write standard SQL to query any table, and the underlying Serverless SQL Warehouse provides best-in-class performance. You can also review the query profile to troubleshoot performance bottlenecks and see query history.\n",
    "\n",
    "### To Try It Yourself:\n",
    "1. In the left navigation bar, select **SQL Editor**.\n",
    "2. Ensure a SQL Warehouse is selected in the top-right.\n",
    "3. Run the query below to explore the clean, aggregated data produced by one of the demo pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5df511c-0047-46a0-bf40-a6e0c8c42027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is the final 'gold' table from the bike pipeline demo, ready for BI.\n",
    "SELECT * FROM main.dbdemos_pipeline_bike.rides;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b83e6c-4d42-4309-81c7-1802743291ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orchestration Method 1: Databricks Jobs (The Imperative Approach)\n",
    "\n",
    "A **Job** is the simplest way to run a notebook, script, or SQL query on a schedule. This is an *imperative* approach, where you define the steps to be executed in order. Think of it as a powerful \"cron job\" for Databricks, perfect for straightforward, routine tasks.\n",
    "\n",
    "### To See How it Works:\n",
    "\n",
    "You can schedule any notebook to run as a Job by clicking the **Schedule** button in the top-right corner of the notebook UI. This will take you to the Jobs UI where you can define the schedule and compute.\n",
    "\n",
    "##Databricks Workflows: Build, Run, and Manage ETL, ML, and Analytics Video\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/MWGmsnnaGLY/0.jpg)](https://www.youtube.com/watch?v=MWGmsnnaGLY \"Databricks Workflows: Build, Run, and Manage ETL, ML, and Analytics\")\n",
    "\n",
    "ðŸ“– **Resource:** [Databricks Workflows Quickstart](https://docs.databricks.com/en/workflows/jobs/jobs-quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9fac878-10bd-45fd-9181-55e2f8223846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orchestration Method 2: Delta Live Tables (The Declarative Approach)\n",
    "\n",
    "For building robust data pipelines, **Delta Live Tables (DLT)** is the modern, recommended approach. It's a *declarative* framework: instead of defining the *steps* of your pipeline, you simply define the *end state* of your tables using standard SQL or Python.\n",
    "\n",
    "DLT automatically manages the underlying infrastructure, orchestration, data quality monitoring, and error handling.\n",
    "\n",
    "Your setup script already deployed a DLT pipeline from the `pipeline-bike` demo!\n",
    "\n",
    "### To Explore the DLT Pipeline:\n",
    "\n",
    "1. **See the Definition:** In your workspace, navigate to the `pipeline-bike` demo folder and open the **`01-DLT-Pipeline-SQL`** notebook. This is the simple SQL code that defines the entire pipeline.\n",
    "2. **See it Running:** Use the link generated by the setup script (or go to `Workflows > Delta Live Tables`) to see the live pipeline graph. You can monitor data flowing through the bronze, silver, and gold tables and see data quality scores.\n",
    "\n",
    "### What is Delta Live Tables? (Video)\n",
    "\n",
    "[![Video Thumbnail](https://img.youtube.com/vi/rr0I6AMwqS0/0.jpg)](https://www.youtube.com/watch?v=rr0I6AMwqS0 \"What is Delta Live Tables?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50224f0a-4967-46be-80f4-46679dc9378d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced SQL Techniques and Performance Optimization\n",
    "\n",
    "### Query Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f561b023-a4fa-4c48-97fd-1a05b8241f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Example: Using partitioning for large tables\n",
    "CREATE TABLE main.default.sales_data (\n",
    "    transaction_id STRING,\n",
    "    amount DECIMAL(10,2),\n",
    "    customer_id STRING,\n",
    "    sale_date DATE\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (sale_date);\n",
    "\n",
    "-- Example: Analyzing query performance\n",
    "EXPLAIN EXTENDED \n",
    "SELECT customer_id, SUM(amount) as total_sales\n",
    "FROM main.default.sales_data \n",
    "WHERE sale_date >= '2024-01-01'\n",
    "GROUP BY customer_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d719e3-3e9f-4a0e-bd10-2b22dd34d2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comprehensive Resource Library\n",
    "\n",
    "### ðŸ“š **Official Documentation**\n",
    "* [SQL Analytics and Warehousing Complete Guide](https://docs.databricks.com/en/sql/index.html)\n",
    "* [Delta Live Tables Documentation](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "* [Databricks Workflows (Jobs) Guide](https://docs.databricks.com/en/workflows/index.html)\n",
    "* [SQL Reference Guide](https://docs.databricks.com/en/sql/language-manual/index.html)\n",
    "* [Query Optimization Best Practices](https://docs.databricks.com/en/optimizations/index.html)\n",
    "* [Data Quality Monitoring](https://docs.databricks.com/en/delta-live-tables/expectations.html)\n",
    "\n",
    "### ðŸ”§ **Tools and Integrations**\n",
    "* [dbt Integration with Databricks](https://docs.databricks.com/en/partners/prep/dbt.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8684803394761503,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Querying and Orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
